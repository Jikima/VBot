from __future__ import annotations
import datetime
import logging

import tiktoken

import openai

import requests
import json
from datetime import date
from calendar import monthrange

# Models can be found here: https://platform.openai.com/docs/models/overview
GPT_3_MODELS = ("gpt-3.5-turbo", "gpt-3.5-turbo-0301")
GPT_4_MODELS = ("gpt-4", "gpt-4-0314")
GPT_4_32K_MODELS = ("gpt-4-32k", "gpt-4-32k-0314")
GPT_ALL_MODELS = GPT_3_MODELS + GPT_4_MODELS + GPT_4_32K_MODELS


def default_max_tokens(model: str) -> int:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –ª–µ–∫—Å–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
    :param model: –ò–º—è –º–æ–¥–µ–ª–∏
    :return: –ß–∏—Å–ª–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –ª–µ–∫—Å–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    """
    return 1200 if model in GPT_3_MODELS else 2400


class OpenAIHelper:
    """
    –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å ChatGPT.
    """

    def __init__(self, config: dict):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å-–ø–æ–º–æ—â–Ω–∏–∫ OpenAI —Å –∑–∞–¥–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π.
        :param config: –°–ª–æ–≤–∞—Ä—å, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é GPT
        """
        openai.api_key = config['api_key']
        openai.proxy = config['proxy']
        self.config = config
        self.conversations: dict[int: list] = {}  # {chat_id: history}
        self.last_updated: dict[int: datetime] = {}  # {chat_id: last_update_timestamp}

    def get_conversation_stats(self, chat_id: int) -> tuple[int, int]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–µ.
        :param chat_id: ID —á–∞—Ç–∞
        :return: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —Ç–æ–∫–µ–Ω–æ–≤.
        """
        if chat_id not in self.conversations:
            self.reset_chat_history(chat_id)
        return len(self.conversations[chat_id]), self.__count_tokens(self.conversations[chat_id])

    async def get_chat_response(self, chat_id: int, query: str) -> tuple[str, str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ GPT.
        :param chat_id: ID —á–∞—Ç–∞
        :param query: –ó–∞–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ –º–æ–¥–µ–ª—å
        :return: –û—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        """
        response = await self.__common_get_chat_response(chat_id, query)
        answer = ''

        if len(response.choices) > 1 and self.config['n_choices'] > 1:
            for index, choice in enumerate(response.choices):
                content = choice['message']['content'].strip()
                if index == 0:
                    self.__add_to_history(chat_id, role="assistant", content=content)
                answer += f'{index + 1}\u20e3\n'
                answer += content
                answer += '\n\n'
        else:
            answer = response.choices[0]['message']['content'].strip()
            self.__add_to_history(chat_id, role="assistant", content=answer)

        if self.config['show_usage']:
            answer += "\n\n---\n" \
                      f"üí∞ Tokens used: {str(response.usage['total_tokens'])}" \
                      f" ({str(response.usage['prompt_tokens'])} prompt," \
                      f" {str(response.usage['completion_tokens'])} completion)"

        return answer, response.usage['total_tokens']

    async def get_chat_response_stream(self, chat_id: int, query: str):
        """
        –ü–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ GPT.
        :param chat_id: ID —á–∞—Ç–∞
        :param query: –ó–∞–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ –º–æ–¥–µ–ª—å
        :return: –û—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∏–ª–∏ 'not_finished'.
        """
        response = await self.__common_get_chat_response(chat_id, query, stream=True)

        answer = ''
        async for item in response:
            if 'choices' not in item or len(item.choices) == 0:
                continue
            delta = item.choices[0].delta
            if 'content' in delta:
                answer += delta.content
                yield answer, 'not_finished'
        answer = answer.strip()
        self.__add_to_history(chat_id, role="assistant", content=answer)
        tokens_used = str(self.__count_tokens(self.conversations[chat_id]))

        if self.config['show_usage']:
            answer += f"\n\n---\nüí∞ –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–æ–∫–µ–Ω—ã: {tokens_used}"

        yield answer, tokens_used

    async def __common_get_chat_response(self, chat_id: int, query: str, stream=False):
        """
        –ó–∞–ø—Ä–æ—Å –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏ GPT.
        :param chat_id: ID —á–∞—Ç–∞
        :param query: –ó–∞–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ –º–æ–¥–µ–ª—å
        :return: –û—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        """
        try:
            if chat_id not in self.conversations or self.__max_age_reached(chat_id):
                self.reset_chat_history(chat_id)

            self.last_updated[chat_id] = datetime.datetime.now()

            self.__add_to_history(chat_id, role="user", content=query)

            # Summarize the chat history if it's too long to avoid excessive token usage
            token_count = self.__count_tokens(self.conversations[chat_id])
            exceeded_max_tokens = token_count + self.config['max_tokens'] > self.__max_model_tokens()
            exceeded_max_history_size = len(self.conversations[chat_id]) > self.config['max_history_size']

            if exceeded_max_tokens or exceeded_max_history_size:
                logging.info(f'–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –¥–ª—è —á–∞—Ç–∞ ID {chat_id} —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π. –ü–æ–¥–≤–µ–¥–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤...')
                try:
                    summary = await self.__summarise(self.conversations[chat_id][:-1])
                    logging.debug(f'Summary: {summary}')
                    self.reset_chat_history(chat_id)
                    self.__add_to_history(chat_id, role="assistant", content=summary)
                    self.__add_to_history(chat_id, role="user", content=query)
                except Exception as e:
                    logging.warning(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≤–µ–¥–µ–Ω–∏–∏ –∏—Ç–æ–≥–æ–≤ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–æ–≤: {str(e)}. –í—ã—Å–∫–∞–∫–∏–≤–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤–º–µ—Å—Ç–æ...')
                    self.conversations[chat_id] = self.conversations[chat_id][-self.config['max_history_size']:]

            return await openai.ChatCompletion.acreate(
                model=self.config['model'],
                messages=self.conversations[chat_id],
                temperature=self.config['temperature'],
                n=self.config['n_choices'],
                max_tokens=self.config['max_tokens'],
                presence_penalty=self.config['presence_penalty'],
                frequency_penalty=self.config['frequency_penalty'],
                stream=stream
            )

        except openai.error.RateLimitError as e:
            raise Exception(f'‚ö†Ô∏è _–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ OpenAI_ ‚ö†Ô∏è\n{str(e)}') from e

        except openai.error.InvalidRequestError as e:
            raise Exception(f'‚ö†Ô∏è _OpenAI –ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å_ ‚ö†Ô∏è\n{str(e)}') from e

        except Exception as e:
            raise Exception(f'‚ö†Ô∏è _–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞_ ‚ö†Ô∏è\n{str(e)}') from e

    async def generate_image(self, prompt: str) -> tuple[str, str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å DALL-E.
        :param prompt: –ü–æ–¥—Å–∫–∞–∑–∫–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ –º–æ–¥–µ–ª—å
        :return: URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –µ–≥–æ —Ä–∞–∑–º–µ—Ä
        """
        try:
            response = await openai.Image.acreate(
                prompt=prompt,
                n=1,
                size=self.config['image_size']
            )

            if 'data' not in response or len(response['data']) == 0:
                logging.error(f'–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT: {str(response)}')
                raise Exception('‚ö†Ô∏è _–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞_ ‚ö†Ô∏è\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É —á–µ—Ä–µ–∑ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è.')

            return response['data'][0]['url'], self.config['image_size']
        except Exception as e:
            raise Exception(f'‚ö†Ô∏è _–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞_ ‚ö†Ô∏è\n{str(e)}') from e

    async def transcribe(self, filename):
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å Whisper.
        """
        try:
            with open(filename, "rb") as audio:
                result = await openai.Audio.atranscribe("whisper-1", audio)
                return result.text
        except Exception as e:
            logging.exception(e)
            raise Exception(f'‚ö†Ô∏è _–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞_ ‚ö†Ô∏è\n{str(e)}') from e

    def reset_chat_history(self, chat_id, content=''):
        """
        –°–±—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤.
        """
        if content == '':
            content = self.config['assistant_prompt']
        self.conversations[chat_id] = [{"role": "system", "content": content}]

    def __max_age_reached(self, chat_id) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.
        :param chat_id: ID —á–∞—Ç–∞
        :return: –ë—É–ª–µ–≤–æ –∑–Ω–∞—á–µ–Ω–∏–µ, —É–∫–∞–∑—ã–≤–∞—é—â–µ–µ, –±—ã–ª –ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç –±–µ—Å–µ–¥—ã.
        """
        if chat_id not in self.last_updated:
            return False
        last_updated = self.last_updated[chat_id]
        now = datetime.datetime.now()
        max_age_minutes = self.config['max_conversation_age_minutes']
        return last_updated < now - datetime.timedelta(minutes=max_age_minutes)

    def __add_to_history(self, chat_id, role, content):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.
        :param chat_id: ID —á–∞—Ç–∞
        :param role: –†–æ–ª—å –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è
        :param content: –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        """
        self.conversations[chat_id].append({"role": role, "content": content})

    async def __summarise(self, conversation) -> str:
        """
        –û–±–æ–±—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.
        :param conversation: –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        :return: –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
        """
        messages = [
            { "role": "assistant", "content": "–ö—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏—Ç–µ —ç—Ç–æ—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä –≤ 700 —Å–∏–º–≤–æ–ª–∞—Ö –∏–ª–∏ –º–µ–Ω—å—à–µ" },
            { "role": "user", "content": str(conversation) }
        ]
        response = await openai.ChatCompletion.acreate(
            model=self.config['model'],
            messages=messages,
            temperature=0.4
        )
        return response.choices[0]['message']['content']

    def __max_model_tokens(self):
        if self.config['model'] in GPT_3_MODELS:
            return 4096
        if self.config['model'] in GPT_4_MODELS:
            return 8192
        if self.config['model'] in GPT_4_32K_MODELS:
            return 32768
        raise NotImplementedError(
            f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ {self.config['model']} –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞."
        )

    # https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
    def __count_tokens(self, messages) -> int:
        """
        –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.
        :param messages: —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        :return: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∂–µ—Ç–æ–Ω–æ–≤
        """
        try:
            model = self.config['model']
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            encoding = tiktoken.get_encoding("gpt-3.5-turbo")

        if model in GPT_3_MODELS:
            tokens_per_message = 4  # every message follows <|start|>{role/name}\n{content}<|end|>\n
            tokens_per_name = -1  # if there's a name, the role is omitted
        elif model in GPT_4_MODELS + GPT_4_32K_MODELS:
            tokens_per_message = 3
            tokens_per_name = 1
        else:
            raise NotImplementedError(f"""num_tokens_from_messages() –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model}.""")
        num_tokens = 0
        for message in messages:
            num_tokens += tokens_per_message
            for key, value in message.items():
                num_tokens += len(encoding.encode(value))
                if key == "name":
                    num_tokens += tokens_per_name
        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
        return num_tokens
    
    def get_billing_current_month(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–∑ API OpenAI –¥–∞–Ω–Ω—ã–µ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤ —Ç–µ–∫—É—â–µ–º –º–µ—Å—è—Ü–µ.

        :return: –¥–æ–ª–ª–∞—Ä–æ–≤–∞—è —Å—É–º–º–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ç–µ–∫—É—â–µ–º –º–µ—Å—è—Ü–µ
        """
        headers = {
            "Authorization": f"Bearer {openai.api_key}"
        }
        # calculate first and last day of current month
        today = date.today()
        first_day = date(today.year, today.month, 1)
        _, last_day_of_month = monthrange(today.year, today.month)
        last_day = date(today.year, today.month, last_day_of_month)
        params = {
            "start_date": first_day,
            "end_date": last_day
        }
        response = requests.get("https://api.openai.com/dashboard/billing/usage", headers=headers, params=params)
        billing_data = json.loads(response.text)
        usage_month = billing_data["total_usage"] / 100 # convert cent amount to dollars
        return usage_month
